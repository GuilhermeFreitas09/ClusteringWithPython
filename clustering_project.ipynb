{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c435f24",
   "metadata": {},
   "source": [
    "**Author**: Guilherme P. Freitas\n",
    "\n",
    "**Date**: August 4, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0122ba",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook I will present some clustering methods like Agglomerative, Kmeans, PAM, DBSCAN, Optics and GMM. Furthermore, the number of clusters is going to be choosen by using the Elbow and BIC methods and the silhouette tool will help us to determine which model fits the best.\n",
    "\n",
    "Observation: I'm not going to use the gender variable inside the models. I've tried to do this by using the Gower distance, but it was hard to choose the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed8d25",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94efe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130da1e",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929800ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barplot and histograms in Exploratory Data Analysis\n",
    "def plot_hist(df):\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    vec = np.array(df['Gender'].value_counts())\n",
    "    plt.bar([\"Female\", \"Male\"], vec, edgecolor = \"black\")\n",
    "    plt.title(f'Gender distribution', size=20)\n",
    "    plt.ylabel(f'counts', size=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    \n",
    "    for i, var in enumerate(['Age',  'Annual Income (k$)', 'Spending Score (1-100)']):\n",
    "        i += 2\n",
    "        plt.subplot(2, 2, i)\n",
    "        plt.hist(np.array(df[var]), edgecolor = \"black\")\n",
    "        plt.title(f'{var} distribution', size = 20)\n",
    "        plt.xlabel(f'{var}', size=15)\n",
    "        plt.ylabel(f'counts', size=15)\n",
    "        plt.xticks(fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n",
    "        \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1, \n",
    "                        right=0.9, \n",
    "                        top=0.9, \n",
    "                        hspace=0.4)\n",
    "    plt.show() \n",
    "    \n",
    "# function that generates colors for each cluster index\n",
    "def col_graph(n):\n",
    "    colors = [\"yellow\", \"red\", \"blue\", \"green\", \"magenta\", \"lime\"]\n",
    "    if(n in range(0,6)):\n",
    "        return colors[n]\n",
    "    else:\n",
    "        return \"black\"\n",
    "\n",
    "# function to plot scatterplot 3d graphs\n",
    "def plot_graph(colors):\n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.set_ylabel('Annual Income (k$)')\n",
    "    ax.set_zlabel('Spending Score (1-100)')\n",
    "    ax.view_init(15, 18)\n",
    "    ax.scatter(new_df.iloc[0:, 0], new_df.iloc[0:, 1], new_df.iloc[0:, 2], c=colors)\n",
    "    plt.show()\n",
    "    \n",
    "# function that returns filtered data for the last histogram\n",
    "def func(i, var):\n",
    "    data = df.loc[0:, [var, 'kmeans']]\n",
    "    data = data.loc[df['kmeans'] == i]\n",
    "    return data.loc[0:, var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a359e8",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf6b8fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Mall_Customers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMall_Customers.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ClusteringWithPython-KIbi5mbS/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ClusteringWithPython-KIbi5mbS/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ClusteringWithPython-KIbi5mbS/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ClusteringWithPython-KIbi5mbS/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ClusteringWithPython-KIbi5mbS/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ClusteringWithPython-KIbi5mbS/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Mall_Customers.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Mall_Customers.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5484069",
   "metadata": {},
   "source": [
    "The dataset has 4 numerical variables (CustomerID, Annual Income and Spending Score) and a single categorical variable (Gender)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90c048",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e757812",
   "metadata": {},
   "source": [
    "## Quantitative Analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926963f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:,2:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af0220",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d25547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, var in enumerate(['Age',  'Annual Income (k$)', 'Spending Score (1-100)']):\n",
    "    i += 1\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.boxplot(np.array(df[var]))\n",
    "    plt.title(f'{var}', size = 10)\n",
    "    plt.xticks([])\n",
    "    \n",
    "plt.subplots_adjust(wspace = 0.8)\n",
    "    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61374f34",
   "metadata": {},
   "source": [
    "## Paired graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce63fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(df.iloc[:,1:], hue=\"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bf39e",
   "metadata": {},
   "source": [
    "We can see that the figure which express the relationship between \"Spending Score\" and Annual Income suggests possible clustering formations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc3fc6",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059cb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.iloc[0:,2:]\n",
    "cormat = new_df.corr()\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    cormat, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14203d26",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13165797",
   "metadata": {},
   "source": [
    "## Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d33e12",
   "metadata": {},
   "source": [
    "The Elbow method is a great way to indentify the optimal number of clusters. It minimizes the Within Cluster Sum of Squares (WSS), given by the following expression\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{K}\\sum_{i\\in S_{k}}\\sum_{j=1}^{p} (x_{ij}-\\bar{x}_{kj})^{2}\n",
    "$$\n",
    "\n",
    "where for each cluster \"k\", $S_{k}$ is the set of observations and $\\bar{x}_{kj}$ is the jth center variable. We are going to use this method for centroid-based models, density-based models and hierarchical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 15):\n",
    "    kmeanModel = KMeans(n_clusters=i)\n",
    "    kmeanModel.fit(new_df)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(range(1,15), distortions, 'bx-')\n",
    "plt.xlabel('Number of clusters', size = 15)\n",
    "plt.ylabel('Distortion', size = 15)\n",
    "plt.title('Elbow Method', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350e6d8",
   "metadata": {},
   "source": [
    "5 is the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a92f0",
   "metadata": {},
   "source": [
    "## Hierarchical (Agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae0ee2",
   "metadata": {},
   "source": [
    "The agglomerative clustering is a bottom-to-top method, which beggins considering each observation as a individual cluster then start the linking process by using a specif linkage function. Here, we will adopt Ward's criteria, which link the clusters using the sum of the squared distances between the observations in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568afe6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.array(new_df)\n",
    "Z = linkage(X, 'ward') # agglomerative by default\n",
    "plt.figure(figsize=(14,8))\n",
    "dendrogram(Z, color_threshold=200, truncate_mode = 'level', p=5)\n",
    "plt.title('Agglomerative method')\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis='x',which='major', labelsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f485e",
   "metadata": {},
   "source": [
    "X-axis can be a group of customers or a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c355d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_model = AgglomerativeClustering(n_clusters=5, linkage=\"ward\").fit_predict(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d497c",
   "metadata": {},
   "source": [
    "## Centroid-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43995925",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "K-means is a clustering method that cluster n observations into k groups where each observation belongs to te cluster with the nearest mean.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1 - Take a sample of k centroids by first shuffling the dataset and then randomly select K data points.\n",
    "\n",
    "2 - Assign each data point to the closest cluster (centroid).\n",
    "\n",
    "3 - Compute the centroids for each cluster by taking the average of it's data points.\n",
    "\n",
    "4 - Keep iterating until there is no change to the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2743ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanModel = KMeans(n_clusters=5, random_state=0)\n",
    "kmeanModel.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for k in kmeanModel.labels_:\n",
    "    colors.append(col_graph(k))\n",
    "plot_graph(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616741f4",
   "metadata": {},
   "source": [
    "### PAM or K-medoids\n",
    "\n",
    "The PAM - Partition Around Medoids algorithm is very similar to k-means, which makes it also known as k-medoids, as it seeks to find a central element (medoid) within the observations themselves that minimizes the distance between the closest elements and forms a cluster. A medoid j, for example, is given by $c_j \\in (x_1,x_2,...,x_{n-1},x_n)$. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1 - Take a sample of size k (medoids) between all observations.\n",
    "\n",
    "2 - Link each observation inside the database with it's closest medoid.\n",
    "\n",
    "3 - For each cluster, find the element that reproduces the least dissimilarity between the others and take it as the new medoid.\n",
    "\n",
    "4 - Repeats steps 2 and 3 until the medoids don't change from one iteration to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids = KMedoids(n_clusters=5, random_state=0) # 0 seed to reproduction\n",
    "kmedoids.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e08a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for k in kmedoids.labels_:\n",
    "    colors.append(col_graph(k))\n",
    "plot_graph(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224054f",
   "metadata": {},
   "source": [
    "## Density-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da7725",
   "metadata": {},
   "source": [
    "These methods don't require an initial number of clusters, but it needs two parameters to work properly. These parameters are MinPts and $\\epsilon$.\n",
    "\n",
    "- MinPts: represents the minimum number of points that a point must have inside it's neighborhood to be considered as a core.\n",
    "\n",
    "- $\\epsilon$ is the neighborhood radios of a point.\n",
    "\n",
    "If p is a core point, then it forms a cluster with all points that are reachable from it.\n",
    "\n",
    "To choose the best parameters for our model, we are going to use the following ideas:\n",
    "\n",
    "- MinPts: use dimensions * 2 = 6\n",
    "\n",
    "- $\\epsilon$: create a vector of the 6th greater distance from each point to another point. Plot the ordered vector (ASC) and looks for a behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_matrix = euclidean_distances(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = []\n",
    "for i in range(0,200):\n",
    "    n = np.sort(d_matrix[i])\n",
    "    vec.append(n[5])\n",
    "vec = np.sort(vec)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117827d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(1,201)]\n",
    "plt.plot(x, vec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a0805",
   "metadata": {},
   "source": [
    "Minpts = 6 and $\\epsilon$ = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe59125",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "Density-based Spatial Clustering of Applications with Noise (DBSCAN), is an algorithm that groups commom observations in high-density areas and marks points that lies alone in low-density regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=15, min_samples=6)\n",
    "dbscan.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for k in dbscan.labels_:\n",
    "    colors.append(col_graph(k))\n",
    "plot_graph(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895c586",
   "metadata": {},
   "source": [
    "### Optics\n",
    "\n",
    "OPTICS (Ordering Points To Identify the Clustering Structure) takes the same two parameters as DBSCAN, but it doesn't just consider the radius to link observations. In theory, it's better for finding clusters in large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b700cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics = OPTICS(eps=15, min_samples=17).fit(new_df)\n",
    "optics.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a638a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for k in optics.labels_:\n",
    "    colors.append(col_graph(k))\n",
    "plot_graph(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c7589",
   "metadata": {},
   "source": [
    "## Gaussian mixture models\n",
    "\n",
    "\n",
    "In these models we use a mixture of multivariate normal distributions, which has the following density\n",
    "\n",
    "$$\n",
    "f(x_{j} | \\mu, \\Sigma) = \n",
    "\\sum_{p=1}^{k} \\alpha_{p} \\frac{1}{\\sqrt{2\\pi |\\Sigma_{p}|}}exp[-\\frac{1}{2}(x_{j} - \\mu_{p})^{t}\\Sigma_{p}^{-1}(x_{j}-\\mu_{p})]\n",
    "$$\n",
    "\n",
    "where p represents each cluster, $\\alpha_p$ is the weight for pth gaussian distribution, $\\mu$ is a mean vector of the dataset atributtes and $\\Sigma$ is the covariance matrix. \n",
    "\n",
    "To estimate the parameters whe use the Expectation-maximization (EM) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59107f5c",
   "metadata": {},
   "source": [
    "### Bayesian Information Criterion\n",
    "\n",
    "Bayesian Information Criterion can easily help us to define the optimal number of clusters and the best model in model-based clustering. The idea is to find a model from a set of candidates that maximizes the posterior probability of an observation x to belong to a cluster y.\n",
    "\n",
    "We calculate BIC by\n",
    "\n",
    "$$\n",
    "klog(n) - 2\\sum L_{i}\n",
    "$$\n",
    "\n",
    "where n is the sample size, $L_{i}$ represents the log-likelihood of cluster \"i\" and k is the number of free parameters. We are going to use this method to find out the best model and number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining BIC value for each model and saving it into an array.\n",
    "X = np.array(new_df)\n",
    "methds = ['full', 'tied', 'diag', 'spherical']\n",
    "Bic = [[0 for i in range(0,len(methds))] for i in range(1,15)]\n",
    "for i in range(1,15):\n",
    "    for j, m in enumerate(methds):\n",
    "        gm = GaussianMixture(n_components=i, random_state=0, covariance_type=m).fit(X).bic(X)\n",
    "        Bic[i-1][j] = gm\n",
    "        \n",
    "Bic = np.array(Bic)\n",
    "Bic_df = pd.DataFrame(Bic, columns=methds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8b8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in methds:\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.plot([x for x in range(1,15)], Bic_df[col], label=col)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403ae0c",
   "metadata": {},
   "source": [
    "Covariance type as diag and n=5 looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4f72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the GMM model busing n=5 components and type=diag\n",
    "gm = GaussianMixture(n_components=5, random_state=0, covariance_type = 'diag').fit(X).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for k in gm:\n",
    "    colors.append(col_graph(k))\n",
    "plot_graph(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae189342",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d53ff",
   "metadata": {},
   "source": [
    "To pick a model we are going to use silhouette scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb82191",
   "metadata": {},
   "source": [
    "## Silhouette score\n",
    "\n",
    "Given a set of clusters $\\Lambda$, we define the silhouette of an observation \"i\" inside a cluster $\\lambda_k$ as\n",
    "\n",
    "$$\n",
    "    s_{i\\lambda_k} = \\frac{b_i - a_i}{max(b_i, a_i)}\n",
    "$$\n",
    "\n",
    "where $a_i$ is the dissimilarity of i related to the elements of cluster $\\lambda_k$ (which contains i) and $b_i$ is the smallest dissimilarity of i with respect to the elements of another cluster $\\lambda$, that is, $b_i = min_{\\lambda \\neq \\lambda_k} d (i, \\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0902e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Agglomerative: {silhouette_score(new_df, agg_model)}')\n",
    "print(f'K-means: {silhouette_score(new_df,kmeanModel.labels_)}')\n",
    "print(f'K-medoids(PAM): {silhouette_score(new_df,kmedoids.labels_)}')\n",
    "print(f'DBSCAN: {silhouette_score(new_df, dbscan.labels_)}')\n",
    "print(f'Optics: {silhouette_score(new_df, optics.labels_)}')\n",
    "print(f'Gaussian MM: {silhouette_score(new_df,gm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c9c04",
   "metadata": {},
   "source": [
    "Silhouette scores suggests that **K-means is our best fit**. The following code aims to analyze the formed clusters using PCA and Histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784295b",
   "metadata": {},
   "source": [
    "## PCA - Principal Component Analysis\n",
    "\n",
    "It's always a good idea to normalize the data before reducing dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_normalized = preprocessing.normalize(np.array(new_df), norm='l2')\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_normalized)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation and labeling\n",
    "X = pca.transform(X_normalized)\n",
    "X = pd.DataFrame(X, columns = ['PCA1', 'PCA2'])\n",
    "X['cluster'] = kmeanModel.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abbcdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coloring according to each cluster label\n",
    "colors = []\n",
    "for k in kmeanModel.labels_:\n",
    "    colors.append(col_graph(k))\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax = fig.add_subplot()\n",
    "ax.set_xlabel('PCA1 (62%)')\n",
    "ax.set_ylabel('PCA2 (35%)')\n",
    "ax.scatter(X.iloc[0:, 0], X.iloc[0:, 1], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241ba1e",
   "metadata": {},
   "source": [
    "## Distributions of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44071eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans'] = kmeanModel.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.array(['yellow', 'red', 'blue', 'green', 'magenta'])\n",
    "vari = np.array(['Age', 'Annual Income (k$)','Spending Score (1-100)'])\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "for i, var in enumerate(vari):\n",
    "    data_new = [func(i, var) for i in range(0,5)]\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.hist(data_new, histtype='barstacked', color=col, alpha=0.8)\n",
    "    plt.title(f'{var} distribution by cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['kmeans','Gender']).size().reset_index(name='counts')\n",
    "# ['yellow', 'red', 'blue', 'green', 'magenta'] = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9e6e9",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab397b",
   "metadata": {},
   "source": [
    "**Yellow cluster** : Diversity of age. Low annual income (max = 40). Spending score lies down between 0 and 40 (low).\n",
    "\n",
    "**Green cluster** : Age going from 18 to near 60's. Higher annual income  (60 to almost 140). Lower spending score.\n",
    "\n",
    "**Blue cluster** : Diversity of age, but it has more older people than the others. Annual income lies between 40 and 90. Spending scores concentrates in the middle of the graph. \n",
    "\n",
    "**Magenta cluster** : Cluster of younger people, with age between 18 and almost 40. Low annual income. High spending scores.\n",
    "\n",
    "**Red cluster** : Group of customers where age goes from middle of tweenties to almost 45. High annual income (60 to almost 140). High spending scores (60 to 100)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
